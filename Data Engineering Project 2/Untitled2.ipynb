{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16281b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_latest_log_file():\n",
    "    # Define the path for today's log directory\n",
    "    today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    log_dir = os.path.join(\"LOGS\", today_date)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Get all log files in the directory\n",
    "    log_files = [logfile for logfile in os.listdir(log_dir) if logfile.endswith('.log')]\n",
    "    \n",
    "    # If there are no log files, return None\n",
    "    if not log_files:\n",
    "        return None\n",
    "\n",
    "    # Get the full path of the most recently created log file\n",
    "    latest_log_file = max(\n",
    "        [os.path.join(log_dir, f) for f in log_files],\n",
    "        key=os.path.getctime\n",
    "    )\n",
    "    \n",
    "    return latest_log_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8423ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the latest log file or create a new one\n",
    "def setup_logging():\n",
    "    latest_log_file = get_latest_log_file()\n",
    "    \n",
    "    if latest_log_file:\n",
    "        # If there's an existing log file, append to it\n",
    "        logging.basicConfig(\n",
    "            filename=latest_log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        logging.info(\"Appending to existing log file.\")\n",
    "    else:\n",
    "        # Create a new log file\n",
    "        today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        log_dir = os.path.join(\"LOGS\", today_date)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%H-%M-%S\")\n",
    "        new_log_file = os.path.join(log_dir, f\"{today_date}_{timestamp}_data_ingestion.log\")\n",
    "\n",
    "        logging.basicConfig(\n",
    "            filename=new_log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        logging.info(\"Created new log file.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53cc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import requests\n",
    "import io\n",
    "import logging\n",
    "import zipfile\n",
    "\n",
    "# Set up logging for monitoring\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "setup_logging()\n",
    "\n",
    "# Kaggle dataset URL and API key\n",
    "dataset_url = \"https://www.kaggle.com/api/v1/datasets/download/thedevastator/analyzing-credit-card-spending-habits-in-india\"\n",
    "headers = {'Authorization': 'Bearer <your-kaggle-api-key>'}\n",
    "\n",
    "# Chunk size for processing large datasets (adjust based on memory constraints)\n",
    "chunk_size = 10000  # Adjust this as necessary\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    # Start downloading dataset\n",
    "    logging.info(f\"#\"*50)\n",
    "    logging.info(f\"STEP 1 : Extarcting data from kagggle\")\n",
    "    logging.info(f\"#\"*50)\n",
    "    logging.info(\"Starting data download...\")\n",
    "    response = requests.get(dataset_url, headers=headers, stream=True)\n",
    "    response.raise_for_status()  # Raise an error for bad responses\n",
    "\n",
    "    # Unzip the downloaded file in memory and process the contents\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        # List the files in the zip archive\n",
    "        file_list = zip_ref.namelist()\n",
    "        logging.info(f\"Files in the zip archive: {file_list}\")\n",
    "\n",
    "        # Extract the CSV file\n",
    "        with zip_ref.open(file_list[0]) as file:\n",
    "            # Read data in chunks for memory-efficient processing\n",
    "            logging.info(\"Processing the data in chunks...\")\n",
    "            for chunk in pd.read_csv(file, chunksize=chunk_size):\n",
    "#                 print(chunk.columns)\n",
    "#                 print(len(chunk))\n",
    "#                 chunk['transaction_id'] = chunk.index\n",
    "                chunk.rename(columns={'index':'transaction_id','City':'city','Date':'transaction_date',\n",
    "                                     'Card Type':'card_type','Exp Type':'exp_type','Gender':'gender','Amount':'amount'},inplace=True)\n",
    "                chunk['transaction_id']+=1\n",
    "                cleaned_chunk = chunk\n",
    "                final_df = pd.concat([final_df,cleaned_chunk])\n",
    "                # Print cleaned data (for testing)\n",
    "                logging.info(f\"Processed {len(cleaned_chunk)} rows of data.\")\n",
    "                \n",
    "                # Write the cleaned data to stdout for NiFi to pick up\n",
    "        \n",
    "#             final_df.to_csv(sys.stdout, index=False, header=True)  # Exclude header after the first chunk                 \n",
    "            final_df.to_csv('output.csv', index=False, header=True)            \n",
    "    logging.info(\"Data processing and transmission to NiFi completed successfully.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    logging.error(f\"Failed to download dataset: {e}\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    logging.error(\"No data found in the response.\")\n",
    "except zipfile.BadZipFile:\n",
    "    logging.error(\"The downloaded file is not a valid zip file.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c54efe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['transaction_id', 'city', 'transaction_date', 'card_type', 'exp_type',\n",
      "       'gender', 'amount'],\n",
      "      dtype='object')\n",
      "       transaction_id                   city transaction_date  card_type  \\\n",
      "0                   1           Delhi, India        29-Oct-14       Gold   \n",
      "1                   2  Greater Mumbai, India        22-Aug-14   Platinum   \n",
      "2                   3       Bengaluru, India        27-Aug-14     Silver   \n",
      "3                   4  Greater Mumbai, India        12-Apr-14  Signature   \n",
      "4                   5       Bengaluru, India         5-May-15       Gold   \n",
      "...               ...                    ...              ...        ...   \n",
      "26047           26048         Kolkata, India        22-Jun-14     Silver   \n",
      "26048           26049            Pune, India         3-Aug-14  Signature   \n",
      "26049           26050       Hyderabad, India        16-Jan-15     Silver   \n",
      "26050           26051          Kanpur, India        14-Sep-14     Silver   \n",
      "26051           26052       Hyderabad, India        19-Oct-13  Signature   \n",
      "\n",
      "      exp_type gender  amount  \n",
      "0        Bills      F   82475  \n",
      "1        Bills      F   32555  \n",
      "2        Bills      F  101738  \n",
      "3        Bills      F  123424  \n",
      "4        Bills      F  171574  \n",
      "...        ...    ...     ...  \n",
      "26047   Travel      F  128191  \n",
      "26048   Travel      M  246316  \n",
      "26049   Travel      M  265019  \n",
      "26050   Travel      M   88174  \n",
      "26051    Bills      M  184410  \n",
      "\n",
      "[26052 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set up logging for monitoring\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "setup_logging()\n",
    "\n",
    "# Database connection parameters\n",
    "server = 'localhost'  \n",
    "database = 'DEProject'  \n",
    "\n",
    "username = 'LAPTOP-HK3BEPAJ\\\\akhil'\n",
    "password = 'your_password_here'\n",
    "\n",
    "# Connect to the database\n",
    "conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;'\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Table name\n",
    "table_name = 'transactions'\n",
    "\n",
    "def table_exists(cursor, table_name):\n",
    "    query = f\"SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = '{table_name}'\"\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchone() is not None\n",
    "\n",
    "def create_table_if_not_exists(cursor, table_name):\n",
    "    if not table_exists(cursor, table_name):\n",
    "        create_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            transaction_id INT PRIMARY KEY,\n",
    "            city VARCHAR(50),\n",
    "            transaction_date DATE,\n",
    "            card_type VARCHAR(20),\n",
    "            exp_type VARCHAR(20),\n",
    "            gender CHAR(1),\n",
    "            amount DECIMAL(10, 2)\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_query)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Table '{table_name}' created.\")\n",
    "    else:\n",
    "        logging.info(f\"Table '{table_name}' already exists.\")\n",
    "\n",
    "def is_unique(cursor, transaction_id, table_name):\n",
    "    check_query = f\"SELECT COUNT(1) FROM {table_name} WHERE transaction_id = ?\"\n",
    "    cursor.execute(check_query, transaction_id)\n",
    "    return cursor.fetchone()[0] == 0\n",
    "\n",
    "def insert_data(cursor, df, table_name):\n",
    "    for _, row in df.iterrows():\n",
    "        if is_unique(cursor, row['transaction_id'], table_name):\n",
    "            insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} (transaction_id, city, transaction_date, card_type, exp_type, gender, amount)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            cursor.execute(insert_query, row['transaction_id'], row['city'], row['transaction_date'],\n",
    "                           row['card_type'], row['exp_type'], row['gender'], row['amount'])\n",
    "            logging.info(f\"Data inserted for transaction ID {row['transaction_id']}.\")\n",
    "        else:\n",
    "            logging.info(f\"Data for transaction ID {row['transaction_id']} already exists.\")\n",
    "\n",
    "# Main process\n",
    "try:\n",
    "    logging.info(f\"#\"*50)\n",
    "    logging.info(f\"STEP 2 : Pushing the data to table\")\n",
    "    logging.info(f\"#\"*50)\n",
    "    create_table_if_not_exists(cursor, table_name)\n",
    "    # Load DataFrame from the flow file (you may need to adjust this)\n",
    "#     df = pd.read_csv(sys.stdin)  # Example for reading from stdin in NiFi\n",
    "    df = pd.read_csv('output.csv')\n",
    "#     print(df.columns)\n",
    "#     print(df)\n",
    "    insert_data(cursor, df, table_name)\n",
    "    conn.commit()\n",
    "    logging.info(\"Data processing and insertion completed successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error processing data: {str(e)}\")\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a104a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15919b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
